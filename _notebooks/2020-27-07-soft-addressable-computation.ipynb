{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: \"Multiplicative interaction\"\n",
    "date: 2020-08-29 15:30:38.122265\n",
    "categories: neural networks, cppn, art\n",
    "draft: True\n",
    "meta:\n",
    "    - https://www.cs.toronto.edu/~hinton/csc2535/notes/lec8a.pdf\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-09-21 17:46:15.438703'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from datetime import datetime\n",
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic feature detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla neural networks have static weights - menaing they do not change after the model has been trained. The weights that weigh representations of the input are always the same. If we think about the weights as the things that define the computation of the network, which they are we have a static computation that always weighs the input the same way.\n",
    "\n",
    "Even thou*, that vanilla deep neural netowkrs are universal function approximators - meaning thay can approximate any continuous* function with an arbitrary deviation - endowing the architecture of the network with specific operations can make it easier to optimize for a specific task.\n",
    "\n",
    "If we want to approximate something resambling computer program behaviour with a neural network we need to invent ways to give the network the ability to do conditional computation - meaning it has to be able to choose a way to transform the input conditioned on representation of the input itself. Giving this ability to the model will make it able to choose different types of computation to be performed for different types of input.\n",
    "\n",
    "Some neural networks already have this kind of computation build into them. These are RNN's and neural networks with attention used somewhere in them. \n",
    "\n",
    "Where are these dynamic multiplicative weights in RNNs and attention based netowrks.\n",
    "\n",
    " - RNNs - if we take the update gate of a standart GRU RNN cell, we have sigmoid activation of the input + the state which is used to interpolate between (menaing it is used multiplicatively) over the new state, which is another function of the input and the state.\n",
    " - In attention we have similar thing - the attention weights that choose how much we attend to different parts of the input or some representation of the input are functions of the input itself.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft addressable computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python37564bitbaseconda450f1bc47f8a4f9f9c4b21d2d568ed8a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "213px",
    "width": "422px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1141px",
    "left": "2103px",
    "right": "20px",
    "top": "146px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
