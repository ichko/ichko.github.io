---
layout: wide-post
title: 1D GAN
subtitle: "Interactive explanation of GANs in single dimension"
date: 2020-10-26 13:38:03 +0200
categories: ml dl neural-networks generators GAN
comments: true
pinned: true
---

<script
  data-main="/assets/one-d-gan/scripts/index.js"
  src="https://requirejs.org/docs/release/2.3.6/minified/require.js">
</script>

{% include templates.html %}

<div class="wrapper">
  <p>
    Click <code class="highlighter-rouge">PLAY</code>
    to see the <code class="highlighter-rouge">GAN</code> training.
  </p>
</div>

<div class="wide-wrapper">
  <div class="gray-box">
    <div class="white-box padding-container flex-horizontal">
      <button class="btn primary fixed-width-100" id="play-pause">Play</button>
      <button class="btn" id="reset">Reset</button>
      <div class="info-box">
        Iteration <br><span id="iteration-info">#0000</span>
      </div>
    </div>

    <loading-indicator id="main-demo">
      <div
        slot="content"
        id="svg-object"
        class="padding-container svg"
        data-url="assets/one-d-gan/diagram.svg"
      ></div>
    </loading-indicator>

  </div>
</div>

<div class="wrapper" markdown="1">

GANs or Generative Adversarial Networks are neural networks
that can be trained to generate data similar to the data in some dataset.
Lets see how they work.

## Two neural networks against one another

GANs work by "pitting" two neural networks against one another. Generator vs Discriminator.

The job of the generator known as $G$ is to learn to map a latent vector $z$
drawn from some known distribution (multivariate Gaussian for instance) to data-point from the dataset.
The job of the Discriminator known as $D$ is to learn to distinguish between the fake
data generated by the generator and the real data points.

The generator and discriminator are two parameterized functions that are
trained with gradient descent. There are different optimization strategies but it generally goes
something like this.

We sample a batch of $z$ vectors and produce fake data points $G(z)$.
Then we take a batch of the real data $X$. We pass both of these batches
trough the $D$ network and end up with $D(G(z))$ and $D(X)$.
The parameters of the discriminator are then updated using gradient
descent as to correct the predictions of the discriminator -
guessing fake (probability of 0) for the generated data and guessing
real (probability of 1) for the real data.

After the discriminator is updated we sample a new batch of $z$s, produce
a new batch of $G(z)$s and pass them trough $D$ - $D(G(z))$. Now
only the generator parameters are updated as if the discriminator
would have guessed the data is real.

Here are the two updates we do one after the other

$$
  \begin{equation*}
  \begin{gathered}
  \begin{aligned}
    D_{loss} &= \frac{1}{2} BCE(D(G(z)), 0) + \frac{1}{2} BCE(D(X), 1)
    \\
    G_{loss} &= BCE(D(G(z)), 1)
    \\
    \\
    \theta_D &= \theta_D - \alpha \nabla_{\theta_D}{D_{loss}}
    \\
    \theta_G &= \theta_G - \alpha \nabla_{\theta_G}{G_{loss}}
  \end{aligned}
  \end{gathered}
  \end{equation*}
$$

where $BCE$ is the binary cross-entropy loss and $\alpha$ is
the learning rate. This process, after multiple iterations
might lead to the generator learning to generate data points
drawn from the distribution of the dataset.

## GAN in a single dimension

The functions $G$ and $D$ in the demo above are multilayer perceptrons
from $R^1$ to $R^1$, meaning the first and the last layer are
always single-dimensional.

Basically, they look like this:

![1D Perceptrons](../assets/one-d-gan/1d-perceptrons.svg)

On the interactive demo we ca clearly see them plotted as
simple functions on the Cartesian plane.

Since we are in $R^1$ we can also see the data plotted as a histogram.
The target dataset is clearly drawn from a mixture of two normal distributions.
The bimodal nature of the target makes it easy to observe and see when and how
the generator has learned to map the input's unimodal distribution to the
target's bimodal density function.
If the training is successful the $G$ function ends up looking something like this

</div>
<div class="wide-wrapper" markdown="1">

<iframe
  src="https://www.desmos.com/calculator/0key5prihw?embed"
  width="100%"
  height="450px"
  style="border: 3px solid #eee"
  frameborder=0
></iframe>

<div class="fig" markdown="1">
  **Fig. 1:** Splitting the distribution. Interactive [demo in DESMOS](https://www.desmos.com/calculator/0key5prihw).
</div>

</div>
<div class="wrapper" markdown="1">

that is if the training does not end up in **mode collapse**.

## Mode collapse

Mode collapse is when the generator is stuck at generating
only one of the modes of your data. In our example that would
mean that the generator gets stuck at mapping the input unimodal
distribution to only one of the modes of the data. This might
look something like this:

<img
  class="center-image"
  style="border: 3px solid #eee"
  src="/assets/one-d-gan/mode-collapse.svg"
/>

<div class="fig" markdown="1">
  **Fig. 2:** 1D Mode collapse.
</div>

What would this mean in higher dimensions? Let's say
you want to generate the MNIST digits and your input
distribution is `32` dimensional Gaussian. The different modes
of the data would correspond to the different digits
in the dataset. You can imagine these modes as `784` dimensional
clouds of points. The clouds saturate at 10 positions corresponding
to the 10 digits. Each digit is similar to the other digits of the same
class, but has some variation in it. So mode collapse in that case
would mean that the generator gets stuck at mapping your input to a
subset of all the modes in your data.

_How do we fight against that - try googling `mode collapse in GANs`!_

## Conclusion

This was a short and interactive introduction to the concept of
generative adversarial networks in the context of one dimension. To learn mode look at the resources listed below.

- You can find the code for the interactive demo [here](https://github.com/ichko/ichko.github.io/blob/master/assets/one-d-gan/scripts/gan.js)!
- And [here](https://github.com/ichko/ml-playground/blob/master/notebooks/GAN_over_single_dim_bi_modal_data.ipynb) you can see the same experiment
  done in Jupyter notebook (PyTorch ahead).

<video class="center-image" controls autoplay="autoplay" loop="">
  <source src="https://ichko.github.io/ml-playground/notebooks/distribs3.webm">
  Your browser does not support the video tag.
</video>

## Resources and Tools

- [Ian Goodfellow: Generative Adversarial Networks (NIPS 2016 tutorial)](https://www.youtube.com/watch?v=HGYYEUSm-0Q)
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)
- [Desmos.com](https://www.desmos.com/) - tool used to plotting
- [Diagrams.net](https://diagrams.net/) - used to generate the diagrams
- [TensorFlowJS](https://www.tensorflow.org/js) - deep learning in the browser

</div>
