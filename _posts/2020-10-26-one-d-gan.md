---
layout: wide-post
title: 1D GAN
date: 2020-10-26 13:38:03 +0200
categories: ml dl neural-networks generators GAN
comments: false
---

<script
  data-main="/assets/one-d-gan/scripts/index.js"
  src="https://requirejs.org/docs/release/2.3.6/minified/require.js">
</script>

{% include templates.html %}

<div class="wrapper">
  <p>
    Click <code class="highlighter-rouge">PLAY</code>
    to see the <code class="highlighter-rouge">GAN</code> training.
  </p>
</div>

<div class="wide-wrapper">
  <div class="gray-box">
    <div class="white-box padding-container flex-horizontal">
      <button class="btn primary fixed-width-100" id="play-pause">Play</button>
      <button class="btn" id="reset">Reset</button>
      <div class="info-box">
        Iteration <br><span id="iteration-info">#0000</span>
      </div>
    </div>

    <loading-indicator id="main-demo">
      <div
        slot="content"
        id="svg-object"
        class="padding-container svg"
        data-url="assets/one-d-gan/diagram.svg"
      ></div>
    </loading-indicator>

  </div>
</div>

<div class="wrapper">
{% markdown %}

GANs or Generative Adversarial Networks are neural networks
that can be trained to generate data similar to the data in some dataset.
Lets see how they work.

# Two neural networks against each other

GANs work by "pitting" two neural networks against each other. Generator vs Discriminator.

The job of the generator known as $G$ is to learn to map a latent vector $z$
drawn from some known distribution (multivariate Gaussian for instance) and generate data-point from the dataset.
The job of the Discriminator known as $D$ is to learn to distinguish between the fake
data generated by the generator and the real coming from the dataset.

The generator and discriminator are two parameterized functions that are
trained with gradient descent. There are different optimization strategies but it generally goes
something like this.

We sample a batch of $z$ vectors and produce fake data-points $G(z)$.
Then we take a batch of the real data $X$. We pass both of these batches
trough the $D$ network and end up with $D(G(z))$ and $D(X)$.
The parameters of the discriminator are the updated using gradient
descent as to correct the predictions of the discriminator -
guessing fake (probability of 0) for the generated data and guessing
real (probability of 1) for the real data.

After the discriminator is updated we sample new batch of $z$s, produce
new batch of $G(z)$s and pass them trough $D$ - $D(G(z))$. Now
only the generator parameters are updated as if the discriminator
would have guessed the data is real.

Here are the two updates we do one after the other

$$
  \begin{aligned}
    &D_{loss} = \frac{1}{2} BCE(D(G(z)), 0) + \frac{1}{2} BCE(D(X), 1)
    \\
    &G_{loss} = BCE(D(G(z)), 1)
    \\
    \\
    &\theta_D = \theta_D - \alpha \nabla_{\theta_D}{D_{loss}}
    \\
    &\theta_G = \theta_G - \alpha \nabla_{\theta_G}{G_{loss}}
  \end{aligned}
$$

where $BCE$ is the binary cross-entropy loss and $\alpha$ is
the learning rate.

{% endmarkdown %}

</div>
