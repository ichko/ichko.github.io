---
layout: widePost
title: "Visualizing VAE with Efemarai"
date: 2020-11-13 13:38:03 +0200
categories: ml dl tutorial vae ae auto-encoder
comments: true
version: 2
---

<div class="wide-wrapper">

<p>
    <img
        src="/assets/visualizing-vae-with-efemarai-v1/efem-train.gif"
        class="center-image hundred-width gray-border"
    />
</p>


</div>
<div class="wrapper">

<p>
    In this tutorial, we are going to implement and visualize the
    training process of <b>Variational Auto-Encoder</b> with
    ðŸ”— <a target="blank" href="http://efemarai.com/">Efemarai</a>.
</p>

<h2>About Efemarai</h2>

<p>
    <b>Efemerari</b> is a visualization and debugging tool for <b>pytorch</b> models.
    It works by scanning the model's forward and backward steps and visualizing
    the different modules, tensors and activations within a 3D environment right
    in your browser.

    Efemerari allows you to inspect all values of each tensor within the model
    computation.
</p>

</div>
<div class="wide-wrapper">

    <p>
        <object
            class="center-image hundred-width"
            type="image/svg+xml"
            data="/assets/visualizing-vae-with-efemarai-v2/efem-present.svg"
        >
            <img
                class="center-image hundred-width"
                src="/assets/visualizing-vae-with-efemarai-v2/efem-present.svg"
            />
            SVG not supported
        </object>
    </p>


</div>
<div class="wrapper">

<h2>What is VAE</h2>

<p>
    Variational auto encoders are neural networks that learn to
    generate unseen data from a dataset by learning to map
    some known (latent) distribution to the unknown distribution of the
    datasets. The architecture of a VAE follows the general architecture
    of an auto-encoder, but also adds special module for sampling from the
    encoded distribution parameters. The optimization procedure of a VAE also
    adds special term to the loss to force the latent distribution to behave
    like a normal distribution.
</p>

<p>
    The following is a diagram of a VAE:
</p>

</div>
<div class="wide-wrapper">

<p>
    <img src="/assets/visualizing-vae-with-efemarai-v1/vae.svg" class="center-image hundred-width" />
</p>

</div>

<hr />

<div class="wrapper">

    <h2>Implementation</h2>

    <p>
        Let's get our hands dirty with some code.
    </p>

    <h4>1. The model</h4>

<p>
    The implementation will be done in <a href="https://pytorch.org/">Pytorch</a>.
    We start by creating a torch module and add two
    <code class="highlighter-rouge">nn.Sequential</code>
    sub-modules for the <b>encoder</b> and the <b>decoder</b>.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    def __init__(self, input_shape, encoding_size):
        super(VAE, self).__init__()

        self.input_shape = input_shape
        flatten_size = np.prod(list(input_shape))
        encoding_size = encoding_size

        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flatten_size, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, encoding_size * 2),
        )

        self.decoder = nn.Sequential(
            nn.Linear(encoding_size, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, flatten_size),
            nn.Sigmoid(),
        )

        self.loss = nn.BCELoss(reduction='mean')

    def forward(self, x):
      pass
{% endhighlight %}
</p>

<p>
    Since the encoder is regressing to the parameters of some
    normal distribution we are going to output
    <code class="highlighter-rouge">encoding_size * 2</code>
    number of parameters for $\sigma$ and $\mu$.

    Let's now add functions for encoding and for decoding data.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def decode(self, x):
        o = self.decoder(x)
        return o.reshape(-1, *self.input_shape)

    def encode(self, x):
        o = self.encoder(x)
        mu, log_sig = torch.chunk(o, 2, dim=1)
        return mu, log_sig
{% endhighlight %}
</p>

<p>
    Notice how we <b>chunk</b> the output of the encoder
    into two vectors of parameters. These vectors correspond to the
    mean and variance of the distribution we are sampling from.
    Then we need a way to sample $z$.
</p>

<h4>2. Forward sample</h4>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def sample(self, mu, log_sig):
        s = torch.normal(0, 1, mu.shape).to(DEVICE)
        return mu + s * torch.exp(log_sig / 2)
{% endhighlight %}
</p>

<p>
    The thing we do in the sample method is called reparameterization and
    it allows us to make the sampling differentiable. Instead of directly
    generating a sample from the inferred parameters we sample a
    normally distributed vector from $N(0,1)$ and
    rescale and shift it accordingly.
</p>
<p>
    And finally, lets implement the forward method.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def forward(self, x):
        mu, log_sig = self.encode(x)
        s = self.sample(mu, log_sig)
        out = self.decode(s)
        return out
{% endhighlight %}
</p>

<h4>3. Dataset</h4>

<p>
    Now we need a dataset. Lets go with the classic - <b>MNIST</b>.
    Luckily <b>torchvision</b> makes this super easy.
</p>

<p>
{% highlight python %}
dataset = torchvision.datasets.MNIST(
    root='./data',
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x[0])
    ]),
    download=True
)

mk_data_loader = lambda bs: torch.utils.data.DataLoader(
    dataset=dataset,
    batch_size=bs,
    shuffle=True
)
{% endhighlight %}
</p>

<h4>4. Optimization loop</h4>

<p>
    After that we initialize our model and we are ready to toss it in whatever training loop framework we like.
    In my case that is this simple optimization function.
</p>

<p>
{% highlight python %}
def optimize(model, data, epochs, lr=0.01, on_it=lambda _: None):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    tr = trange(epochs)
    for epoch in tr:
        for i, (X, y) in tqdm(
            enumerate(data),
            total=len(data),
            desc='Epoch [%i/%i]' % (epoch + 1, epochs)
        ):
            X = X.to(DEVICE)

            optimizer.zero_grad()
            loss = model.criterion(X)
            loss.backward()
            optimizer.step()

            tr.set_description('Loss %.6f' % l)
{% endhighlight %}
</p>

<p>
    We are ready to optimize!
</p>

<p>
{% highlight python %}
loader = mk_data_loader(bs=64)
vae = VAE(X[0].shape, emb_size).to(DEVICE)

optimize(
    model=vae,
    data=loader,
    epochs=5,
    lr=0.01
)
{% endhighlight %}
</p>

<h2>Visualizing the training with Efemarai</h2>

<p>
    Lets install the python package of <i>efemarai</i>:
</p>

<p>
{% highlight bash %}
export EFEMARAI_LICENSE_TOKEN=<your-license-token>

pip install efemarai \
  --extra-index-url https://${EFEMARAI_LICENSE_TOKEN}@pypi.efemarai.com
{% endhighlight %}
</p>

<p>
    After that we are ready to launch the <i>efemarai demon</i>.
</p>

<blockquote>
    <p>
        Running the daemon locally ensures that none of your data,
        code or models leave your computer.
    </p>
</blockquote>

<p>
{% highlight bash %}
efemarai

> Daemon started successfully (use Ctr+C to exit).
{% endhighlight %}
</p>

<p>
    After that to visualize the whole computation graph during training with
    <i>Efemarai</i>, we just have to execute the model backwards step in
    <code class="highlighter-rouge">ef.scan</code> like so:
</p>

<p>
{% highlight python %}
import efemarai as ef

def optimize(...):
    ...

    with ef.scan(wait=i==0):
        loss = model.criterion(X)
        loss.backward()

    ...
{% endhighlight %}
</p>

<p>
    In my case I add <code class="highlight-rouge">wait=i==0</code>,
    to make the execution of the code break on the first iteration.
    This ensures that we can expand the graph in the
    web interface before the training begins.
</p>

<p>
    Now is the time for us to navigate to the web interface of Efemarai at
    <a href="https://app.efemarai.com/run">app.efemarai.com/run</a>.
    You should be greeted with the following screen:
</p>

<p>
    <img src="/assets/visualizing-vae-with-efemarai-v1/efem-first-launch.png" class="center-image hundred-width gray-border" />
</p>

<p>
    You can drag with left mouse click while holding <i>Shift</i>
    to rotate and drag with right mouse click with <i>Shift</i> to pan.
    Click on the blue boxes (operations) on the graph to expand them.
</p>

<h4>Step by step in the web interface</h4>

</div>
<div class="wide-wrapper">

<p>
    <object
        class="center-image hundred-width"
        type="image/svg+xml"
        data="/assets/visualizing-vae-with-efemarai-v1/tutorial.svg"
        >
        SVG not supported
    </object>
</p>

<hr />

<p>
    <img src="/assets/visualizing-vae-with-efemarai-v1/efem-train.gif" class="center-image hundred-width gray-border" />
</p>

</div>
<div class="wrapper">

<b>Running the computation you should observe something similar to this. Pretty cool, huh! ðŸ˜Ž</b>

<h2>Conclusion</h2>

<p>
    In this post we saw what VAE is, how to implement one and how to 
    visualize it. I encourage you to try the experiment yourself
    and play around with the visualization interface of Efemarai. You can
    also try it with models you have already implemented. The code necessary
    for the visualization to work is literally a few lines.
</p>

</div>
