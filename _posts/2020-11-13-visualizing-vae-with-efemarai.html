---
layout: widePost
title: "Visualizing VAE with Efemarai"
date: 2020-11-13 13:38:03 +0200
categories: ml dl tutorial vae ae auto-encoder
comments: true
---

<div class="wrapper">

<p>
    <img src="/assets/vae-efem-tutorial/efem-train.gif" class="center-image hundred-width styled-border" />
    <!-- <img src="https://efemarai.com/wp-content/uploads/2020/08/efemarai_model_home_1024_1.png" class="center-image hundred-width styled-border" /> -->
    
</p>

</div>
<div class="wrapper">

<p>
    In this tutorial, we are going to implement and visualize the training process of <b>Variational Auto-Encoder</b> with üîó <a target="blank" href="http://efemarai.com/">Efemarai</a>.
</p>

<h2>What is VAE</h2>

<h3>Auto-Encoder</h3>

<p>
    To explain what a VAE is we must first explain the general architecture of an auto-encoder.
    Auto-encoders are are a type of neural network architecture that is designed to do dimensionality reduction.
    The network is a composition of an encoder and a decoder. The job of the encoder is to take an input and ‚Äúcompress‚Äù
    it to a vector with less dimensionality. The job of the decoder is to take this lower dimensionality representation and
    expand it to the original shape of the data. The auto-encoder is then trained in unsupervised manned to output whatever
    data we input into it.
    The point of this is for the encoder to learn to squeeze the original data in vectors with lower dimensions
    preserving only the most important features of the data - the ones that can then be used to reconstruct the data with
    higher precision by the decoder.
</p>

</div>
<div class="wide-wrapper">

<p>
    <object class="center-image hundred-width" type="image/svg+xml" data="/assets/vae-efem-tutorial/ae.svg">
    SVG not supported
    </object>
</p>

</div>
<div class="wrapper">

<h3>Variational Auto-Encoder</h3>

<p>
    After seeing what an AE is let‚Äôs see how the variation of a VAE comes into play.
    Let‚Äôs say we want to generate new data after we have trained our auto-encoder. We can do that by sampling latent vectors and passing them to the decoder, but we don‚Äôt know the distribution of the latent dimension. We don‚Äôt know how to get latent vectors that are going to be decoded into meaningful outputs.
    This is what the VAE paper addresses. With a VAE instead of learning to encode a fixed vector from which to decode, we learn to encode the parameters of a normal distribution. Using a KL term in the loss function we can force the distribution to be a multivariate normal distribution with mean zero and variance one.
</p>

<p>
    KL stands for <i>Kullback‚ÄìLeibler</i> divergence which is a measure of distance between two distributions. Minimizing the KL loss in addition to the reconstruction loss lets us have an auto-encoder with known (normal) latent distribution, which we can then sample from and generate new data points by decoding the sampled vectors.
</p>

</div>
<div class="wide-wrapper">

<p>
    <img src="/assets/vae-efem-tutorial/vae.svg" class="center-image hundred-width" />
</p>

</div>
<hr />
<div class="wrapper">

<h2>Implementation</h2>

<h4>1. The model</h4>

<p>
    The implementation will be done in <a href="https://pytorch.org/">Pytorch</a>. We start by creating a torch module and add two
    <code class="highlighter-rouge">nn.Sequential</code> sub-modules for the <b>encoder</b> and the <b>decoder</b>.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    def __init__(self, input_shape, encoding_size):
        super(VAE, self).__init__()

        self.input_shape = input_shape
        flatten_size = np.prod(list(input_shape))
        encoding_size = encoding_size

        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flatten_size, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, encoding_size * 2),
        )

        self.decoder = nn.Sequential(
            nn.Linear(encoding_size, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, flatten_size),
            nn.Sigmoid(),
        )

        self.loss = nn.BCELoss(reduction='mean')

    def forward(self, x):
      pass
{% endhighlight %}
</p>

<p>
    Since the encoder is regressing to the parameters of some normal distribution we are going to
    output <code class="highlighter-rouge">encoding_size * 2</code> number of parameters for $\sigma$ and $\mu$.

    Let's now add functions for encoding and for decoding data.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def decode(self, x):
        o = self.decoder(x)
        return o.reshape(-1, *self.input_shape)

    def encode(self, x):
        o = self.encoder(x)
        mu, log_sig = torch.chunk(o, 2, dim=1)
        return mu, log_sig
{% endhighlight %}
</p>

<p>
    Notice how we <b>chunk</b> the output of the encoder into two vectors of parameters.
    Then we need a way to sample $z$.
</p>

<h4>2. Forward sample</h4>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def sample(self, mu, log_sig):
        s = torch.normal(0, 1, mu.shape).to(DEVICE)
        return mu + s * torch.exp(log_sig / 2)
{% endhighlight %}
</p>

<p>
    And finally, lets implement the forward method.
</p>

<p>
{% highlight python %}
class VAE(nn.Module):
    ...

    def forward(self, x):
        mu, log_sig = self.encode(x)
        s = self.sample(mu, log_sig)
        out = self.decode(s)
        return out
{% endhighlight %}
</p>

<h4>3. Dataset</h4>

<p>
    Now we need a dataset. Lets go with the classic - <b>MNIST</b>. Luckily pytorch makes
    this super easy.
</p>

<p>
{% highlight python %}
dataset = torchvision.datasets.MNIST(
    root='./data',
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x[0])
    ]),
    download=True
)

mk_data_loader = lambda bs: torch.utils.data.DataLoader(
    dataset=dataset,
    batch_size=bs,
    shuffle=True
)
{% endhighlight %}
</p>

<h4>4. Optimization loop</h4>

<p>
    After that we initialize our model and we are ready to toss it in whatever training loop framework we like.
    In my case that is this simple optimization function.
</p>

<p>
{% highlight python %}
def optimize(model, data, epochs, lr=0.01, on_it=lambda _: None):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    tr = trange(epochs)
    for epoch in tr:
        for i, (X, y) in tqdm(
            enumerate(data),
            total=len(data),
            desc='Epoch [%i/%i]' % (epoch + 1, epochs)
        ):
            X = X.to(DEVICE)

            optimizer.zero_grad()
            loss = model.criterion(X)
            loss.backward()
            optimizer.step()

            tr.set_description('Loss %.6f' % l)
{% endhighlight %}
</p>

<p>
    We are ready to optimize!
</p>

<p>
{% highlight python %}
loader = mk_data_loader(bs=64)
vae = VAE(X[0].shape, emb_size).to(DEVICE)

optimize(
    model=vae,
    data=loader,
    epochs=5,
    lr=0.01
)
{% endhighlight %}
</p>

<p>
    // TODO: Add optimization gif
</p>

<h2>Visualizing the training with Efemarai</h2>

<p>
    Lets install the python package of <i>efemarai</i>:
</p>

<p>
{% highlight bash %}
export EFEMARAI_LICENSE_TOKEN=<your-license-token>

pip install efemarai \
  --extra-index-url https://${EFEMARAI_LICENSE_TOKEN}@pypi.efemarai.com
{% endhighlight %}
</p>

<p>
    After that we are ready to launch the <i>efemarai demon</i>.
</p>

<blockquote>
    <p>
        Running the daemon locally ensures that none of your data, code or models leave your computer.
    </p>
</blockquote>

<p>
{% highlight bash %}
efemarai

> Daemon started successfully (use Ctr+C to exit).
{% endhighlight %}
</p>

<p>
    After that to visualize the whole computation graph during training with <i>Efemarai</i>, we just have to execute the model
    backwards step in <code class="highlighter-rouge">ef.scan</code> like so:
</p>

<p>
{% highlight python %}
import efemarai as ef

def optimize(...):
    ...

    with ef.scan(wait=i==0):
        loss = model.criterion(X)
        loss.backward()

    ...
{% endhighlight %}
</p>

<p>
    In my case I add <code class="highlight-rouge">wait=i==0</code>, to make the execution of the code break on the first iteration.
    This ensures that we can expand the graph in the web interface before the training begins.
</p>

<p>
    Now is the time for us to navigate to the web interface of Efemarai at <a href="https://app.efemarai.com/run">app.efemarai.com/run</a>.
    You should be greeted with the following screen:
</p>

<p>
    <img src="/assets/vae-efem-tutorial/efem-first-launch.png" class="center-image hundred-width styled-border" />
</p>

<p>
    You can drag with left mouse click while holding <i>Shift</i> to rotate and drag with right mouse click with <i>Shift</i> to pan.
    Click on the blue boxes (operations) on the graph to expand them.
</p>

<h4>Step by step in the web interface</h4>

</div>
<div class="wide-wrapper">

<p>
    <object class="center-image hundred-width" type="image/svg+xml" data="/assets/vae-efem-tutorial/tutorial.svg">
    SVG not supported
    </object>
</p>

<hr />

<p>
    <img src="/assets/vae-efem-tutorial/efem-train.gif" class="center-image hundred-width styled-border" />
</p>

</div>
<div class="wrapper">

<b>Running the computation you should observe something similar to this. Pretty cool, huh! üòé</b>

<h2>Conclusion</h2>

<p>
    // TODO:
</p>

</div>
